{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower-Recognition-Challenges.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15-SrDUSQaTQfJJRr-m2CbeLEnOAvdABB",
      "authorship_tag": "ABX9TyPaD7ha715vSXhxfa0o7S51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/CV-Object-Detection-Projects/blob/main/Flower_Recognition_Challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flowers Recognition**\n",
        "It is a dataset from kaggle and I will implement some of the best notebooks and compare them to learn the best pattern to solve this problem.\n",
        "you can access the dataset or the notebooks in [here](https://www.kaggle.com/datasets/alxmamaev/flowers-recognition)"
      ],
      "metadata": {
        "id": "1aZen4Gyyr-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, model_selection, metrics"
      ],
      "metadata": {
        "id": "yvUh2IGTxv0v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data"
      ],
      "metadata": {
        "id": "EMJbTD53TqfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/'\n",
        "!kaggle datasets download -d alxmamaev/flowers-recognition\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "id": "y2LYYXBQTpva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_folder = '/content/flowers'\n",
        "\n",
        "size = 224\n",
        "data = []\n",
        "label = []\n",
        "data_names = []\n",
        "id = 0\n",
        "for folder in os.listdir(path_folder):\n",
        "  for files in os.listdir(os.path.join(path_folder, folder)):\n",
        "    if files.endswith('jpg'):\n",
        "      label.append(folder)\n",
        "      img_path = os.path.join(path_folder, folder, files)\n",
        "      data_names.append((id, img_path))\n",
        "      img = cv2.imread(img_path)\n",
        "      img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      im = cv2.resize(img_rgb, (size, size))\n",
        "      data.append(im)\n",
        "      id += 1\n",
        "    else:\n",
        "      continue"
      ],
      "metadata": {
        "id": "VKuMQ_DSVF29"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(label), len(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf8c0U7FXceQ",
        "outputId": "81ba0389-52c7-4b86-ce06-fcf3b5816499"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'daisy', 'dandelion', 'rose', 'sunflower', 'tulip'}, 4317)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cKZT8X0XgKu",
        "outputId": "9d733a94-a11a-4dda-85bf-8b341557b9e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4317, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(data)\n",
        "label = np.array(label)"
      ],
      "metadata": {
        "id": "aW6SCiQhXgHn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data/255\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = encoder.fit_transform(label)\n",
        "y = tf.keras.utils.to_categorical(y)"
      ],
      "metadata": {
        "id": "szesPTsMXjSD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating test, train data from x, will consume all them memory we have in colab,\n",
        "so instead we can put **data_names** in to train/test splits.\n"
      ],
      "metadata": {
        "id": "5o7Z_7c8Ddpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, random_state=32, test_size=0.2)\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(data_names, y, random_state=32, test_size=0.2)"
      ],
      "metadata": {
        "id": "YL1P4-FEcha9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train), len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REMebpcbc_Wi",
        "outputId": "4213abef-11f0-436e-e47a-6bbf8daa84fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3453, 864)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUpQd0jnJwS1",
        "outputId": "e16f1f10-a1e5-4e5e-ca5e-8f9f82450a56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(270, '/content/flowers/dandelion/4610125337_50798408b8_m.jpg'),\n",
              " (3088, '/content/flowers/daisy/2578695910_5ab8ee17c1_n.jpg'),\n",
              " (3784, '/content/flowers/sunflower/1043442695_4556c4c13d_n.jpg'),\n",
              " (238, '/content/flowers/dandelion/34690479536_69da7b98e7_n.jpg'),\n",
              " (1325, '/content/flowers/rose/26811158051_8f264eea6b_n.jpg'),\n",
              " (3828, '/content/flowers/sunflower/3568925290_faf7aec3a0.jpg'),\n",
              " (3313, '/content/flowers/daisy/34476770012_38fba290f3_n.jpg'),\n",
              " (911, '/content/flowers/dandelion/2330339852_fbbdeb7306_n.jpg'),\n",
              " (4228, '/content/flowers/sunflower/14678298676_6db8831ee6_m.jpg'),\n",
              " (8, '/content/flowers/dandelion/7267547016_c8903920bf.jpg'),\n",
              " (3030, '/content/flowers/daisy/144076848_57e1d662e3_m.jpg'),\n",
              " (593, '/content/flowers/dandelion/6901435398_b3192ff7f8_m.jpg'),\n",
              " (3547, '/content/flowers/daisy/2838487505_6c3b48efa5_m.jpg'),\n",
              " (464, '/content/flowers/dandelion/17322195031_c2680809dc_m.jpg'),\n",
              " (1564, '/content/flowers/rose/12406229175_82e2ac649c_n.jpg'),\n",
              " (2783, '/content/flowers/tulip/19950169968_a29db804b5_n.jpg'),\n",
              " (1080, '/content/flowers/rose/5181899042_0a6ffe0c8a_n.jpg'),\n",
              " (1416, '/content/flowers/rose/4654893119_45d232016b.jpg'),\n",
              " (3298, '/content/flowers/daisy/3506866918_61dd5fc53b_n.jpg'),\n",
              " (508, '/content/flowers/dandelion/34533729452_7d8e2b519d_n.jpg')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.CNN Architectures : VGG, ResNet, Inception + TL\n",
        "[Note Book](https://www.kaggle.com/code/shivamb/cnn-architectures-vgg-resnet-inception-tl)"
      ],
      "metadata": {
        "id": "LBB884Uwp7Va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 VGG16\n",
        "####Visual Geometry Group from Oxford\n",
        "VGG-16 is a convolutional neural network that is 16 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals"
      ],
      "metadata": {
        "id": "sYe3j7OqvYSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data"
      ],
      "metadata": {
        "id": "tTk0TqweTcTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_imgs = np.array([x[1], x[2], x[3], x[4]])"
      ],
      "metadata": {
        "id": "kRbKkONrTeCa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loader(img_path, t_size=224):\n",
        "  img = Image.image.load_img(img_path, target_size=(t_size, t_size))\n",
        "  img = Image.image.img_to_array(img)\n",
        "  img = np.expand_dim(img, axis=0)\n",
        "  img = tf.keras.applications.vgg16.preprocess_input(img)\n",
        "  return img"
      ],
      "metadata": {
        "id": "sEIiwJKgTgsT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_img(imgs, model):\n",
        "  fig, ax = plt.subplots(1, 4)\n",
        "  fig.set_size_inches(80, 40)\n",
        "  for i in range(4):\n",
        "    ax[i].imshow(Image.open(imgs[i][1]).resize((200, 200), Image.ANTIALIAS))\n",
        "    plt.show()\n",
        "  \n",
        "  fig, ax = plt.subplots(1, 4)\n",
        "  fig.set_size_inches(80, 20)\n",
        "  for i, img_path in enumerate(imgs):\n",
        "    img = image_loader(img_path[1])\n",
        "    preds = tf.keras.applications.vgg16.decode_predictions(model.pedict(img), top=3)[0]\n",
        "    b = sns.barplot(y=[c[1] for c in preds], x=[c[2] for c in preds], color='gray', ax=ax[i])\n",
        "    b.tick_params(labelsize=55)\n",
        "    fig.tighy_layout()"
      ],
      "metadata": {
        "id": "N0qizok_Gu2R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg16 = VGG16()"
      ],
      "metadata": {
        "id": "GE5ipW-TGuy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###From Scratch VGG16"
      ],
      "metadata": {
        "id": "8m0uapPC-auP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #1\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name='block1_conv1')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name='block1_conv2')\n",
        "    self.maxpool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
        "    #2\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block2_conv1')\n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name='block2_conv2')\n",
        "    self.maxpool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
        "    #3\n",
        "    self.conv5 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', name='block3_conv1')\n",
        "    self.conv6 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', name='block3_conv2')\n",
        "    self.conv7 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', name='block3_conv3')\n",
        "    self.maxpool3 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
        "    #4\n",
        "    self.conv8 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', name='block4_conv1')\n",
        "    self.conv9 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', name='block4_conv2')\n",
        "    self.conv10 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', name='block4_conv3')\n",
        "    self.maxpool4 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
        "    #5\n",
        "    self.conv11 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', name='block5_conv1')\n",
        "    self.conv12 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', name='block5_conv2')\n",
        "    self.conv13 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu', name='block5_conv3')\n",
        "    self.maxpool5 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
        "    #6\n",
        "    self.flat = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(4096, activation='relu', name='fc1')\n",
        "    self.droput1 = tf.keras.layers.Dropout(rate=0.5, name='Dropout1')\n",
        "    self.dense2 = tf.keras.layers.Dense(4096, activation='relu', name='fc2')\n",
        "    self.droput2 = tf.keras.layers.Dropout(rate=0.5, name='Dropout2')\n",
        "    self.dense3 = tf.keras.layers.Dense(1000, activation='softmax', name='fc2')\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.conv7(x)\n",
        "    x = self.maxpool3(x)\n",
        "\n",
        "    x = self.conv8(x)\n",
        "    x = self.conv9(x)\n",
        "    x = self.conv10(x)\n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "    x = self.conv11(x)\n",
        "    x = self.conv12(x)\n",
        "    x = self.conv13(x)\n",
        "    x = self.maxpool5(x)\n",
        "    \n",
        "    x = self.flat(x)\n",
        "    \n",
        "    x = self.dense1(x)\n",
        "    x = self.droput1(x)\n",
        "\n",
        "    x = self.dense2(x)\n",
        "    x = self.droput2(x)\n",
        "\n",
        "    x = self.dense3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "CmSwf58-01TO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_input = tf.keras.layers.Input((224, 224, 1))\n",
        "vgg16_model = VGG16()\n",
        "vgg16_model(my_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1CNzspSp5av",
        "outputId": "341e4b96-b165-422b-c4dc-7b2b75fa1856"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1000) dtype=float32 (created by layer 'vgg16_12')>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pretrained VGG16"
      ],
      "metadata": {
        "id": "hUQsA65T-hGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHeDl0_jxSVn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}