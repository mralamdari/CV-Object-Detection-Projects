{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower-Recognition-Challenges-part4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hdo9rcA2n6__BYNo3CtccJ6gGIQT7wrO",
      "authorship_tag": "ABX9TyMHU6oU1A4geyqT0r6jnGK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/CV-Object-Detection-Projects/blob/main/Flower_Recognition_Challenges_part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m-yaX8oXa0Zc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import tqdm\n",
        "import torch\n",
        "import mlxtend\n",
        "import warnings\n",
        "import operator\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import model_selection, metrics, preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('always')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "gIRIdfKebJrZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Get Data"
      ],
      "metadata": {
        "id": "hDuXm5QEbPhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d alxmamaev/flowers-recognition\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "id": "pjO6o3X9bLTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch\n",
        "!pip install -q albumentations\n",
        "!pip install -q seaborn\n",
        "!pip install -q tqdm\n",
        "!pip install -q numpy\n",
        "!pip install -q addict"
      ],
      "metadata": {
        "id": "xxTLESAeeQf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import logging\n",
        "from addict import Dict\n",
        "from datetime import datetime, date\n",
        "\n",
        "logging.basicConfig(format=\"[%(levelname)s] [%(asctime)s] - %(message)s\")"
      ],
      "metadata": {
        "id": "4GSchQsReFUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config\n"
      ],
      "metadata": {
        "id": "T4m4cRqlefPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Dict({\n",
        "    \"path\": \"../input/flowers-recognition/flowers/\",\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"batch_size\": 16,\n",
        "    \"augmentations\": A.Compose([\n",
        "        A.Downscale(scale_min=0.6, scale_max=0.99, p=0.2),  \n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1), \n",
        "        A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.5, alpha_coef=0.05, p=0.5), \n",
        "        A.RandomContrast(limit=0.1, p=0.4),\n",
        "        A.RandomGamma(gamma_limit=(50, 150), p=0.4),\n",
        "        A.RandomBrightness(p=0.4),\n",
        "        A.OpticalDistortion(p=0.2),\n",
        "        A.Blur(blur_limit=2, p=0.2),\n",
        "        ToTensor(),\n",
        "    ]),\n",
        "    \"num_workers\": 0,\n",
        "})"
      ],
      "metadata": {
        "id": "Vxxc6J0seeAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowersDataset(Dataset):\n",
        "    def __init__(self, path:str, shape=(256, 256), augmentations=None):\n",
        "        self.__images_classes_pathes = []\n",
        "        self.__shape = shape\n",
        "        self.__augmentations = augmentations\n",
        "        self.labels = []\n",
        "            \n",
        "        if os.path.exists(path):\n",
        "            self.__path = path\n",
        "            folders = [_ for _ in os.listdir(self.__path) if _ != \"flowers\"]\n",
        "\n",
        "            for folder in folders:\n",
        "                folder_path = os.path.join(self.__path, folder)\n",
        "                if os.path.isdir(folder_path) and os.path.exists(folder_path):\n",
        "                    images = os.listdir(folder_path)\n",
        "                    self.labels.append(folder)\n",
        "                    \n",
        "                    \n",
        "                    if len(images):\n",
        "                        for image in images:\n",
        "                            if image.endswith(\"jpeg\") or image.endswith(\"png\") or image.endswith(\"jpg\"):\n",
        "                                image_path = os.path.join(folder_path, image)\n",
        "                                self.__images_classes_pathes.append((folder, image_path))\n",
        "                    else:\n",
        "                        message = f\"Images for folder '{folder}' weren't found!\"\n",
        "                        print(message)\n",
        "                else:\n",
        "                    message = f\"'{folder}' is not folder and it will be skipped!\"\n",
        "                    print(message)\n",
        "                \n",
        "        else:\n",
        "            message = f\"Path '{path}' does not exists!\"\n",
        "            raise Exception(message)\n",
        "    \n",
        "        self.__images_classes_pathes = np.array(self.__images_classes_pathes)\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    def __load_image(self, path, channels=\"RGB\"):\n",
        "        width, height = self.__shape\n",
        "        loader = A.Compose([\n",
        "            A.Resize(width, height),\n",
        "            ToTensor(),\n",
        "        ])\n",
        "        \n",
        "        image = np.array(Image.open(path).convert(channels))\n",
        "        return loader(image=image)[\"image\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.__images_classes_pathes)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.__images_classes_pathes[index]\n",
        "        label, image_path = item\n",
        "        \n",
        "        image = self.__load_image(image_path, channels=\"RGB\")\n",
        "        \n",
        "        if self.__augmentations is not None:\n",
        "            image = self.__augmentations(image=image.permute(1, 2, 0).numpy())[\"image\"]\n",
        "        \n",
        "        label = self.labels.index(label)\n",
        "        \n",
        "        return Dict({\n",
        "            \"label\": label,\n",
        "            \"image\": image\n",
        "        })\n",
        "    \n",
        "    \n",
        "class Trainer:\n",
        "    def __init__(self, model, criterion, optimizer, scheduler=None, metric=None, device=\"cpu\"):\n",
        "        self.__model = model\n",
        "        self.__criterion = criterion\n",
        "        self.__optimizer = optimizer\n",
        "        self.__scheduler = scheduler\n",
        "        self.__metric = metric\n",
        "        self.__device = device\n",
        "        self.logs = Dict({})\n",
        "        \n",
        "        \n",
        "    def __log(self, logs):\n",
        "        for k,v in logs.items():\n",
        "            if k not in self.logs.keys():\n",
        "                self.logs[k] = []\n",
        "                \n",
        "            self.logs[k].append(v)\n",
        "        \n",
        "        \n",
        "    def __make_checkpoint(self, info, path=f\"checkpoints/checkpoint.pt\"):\n",
        "        checkpoint_info = {**info,\n",
        "            \"optimizer_state\": self.__optimizer.state_dict(),\n",
        "            \"model_state\": self.__model.state_dict()}\n",
        "        \n",
        "        torch.save(checkpoint_info, path)\n",
        "    \n",
        "    \n",
        "    def evaluate(self, loader):\n",
        "        loss = 0\n",
        "        length = len(loader)\n",
        "        with torch.no_grad():\n",
        "            loop = tqdm(loader)\n",
        "            loop.set_description(\"Evaluating\")\n",
        "            for batch in loop:\n",
        "                torch.cuda.empty_cache()\n",
        "                images = batch[\"images\"].to(self.__device)\n",
        "                labels = batch[\"labels\"].to(\"cpu\")\n",
        "                        \n",
        "                output = self.__model(images).to(\"cpu\")\n",
        "                        \n",
        "                batch_loss = criterion(output, labels)\n",
        "                loss += batch_loss.item()\n",
        "            \n",
        "        loss /= length\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def save(self, path=\"model.pt\"):\n",
        "        torch.save(self.__model.state_dict(), path)\n",
        "        \n",
        "        \n",
        "    def fit(self, loader, epochs=10, validation_loader=None):\n",
        "        model.to(self.__device)\n",
        "        train_length = len(loader)\n",
        "        \n",
        "        best_validation_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            \n",
        "            loop = tqdm(loader, position=0, leave=True)\n",
        "            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "            for batch in loop:\n",
        "                torch.cuda.empty_cache()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                images = batch[\"images\"].to(self.__device)\n",
        "                labels = batch[\"labels\"].to(\"cpu\")\n",
        "                \n",
        "                output = self.__model(images).to(\"cpu\")\n",
        "                predicted_class = torch.argmax(output, dim=1)\n",
        "        \n",
        "                loss = self.__criterion(output, labels)\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "                \n",
        "                loop.set_postfix(loss=loss.item())\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            epoch_loss /= train_length\n",
        "            \n",
        "            self.__log({\"epochs\": epoch+1, \"train_loss\": epoch_loss})\n",
        "            loop.set_postfix(loss=epoch_loss)\n",
        "            \n",
        "            if validation_loader is not None:\n",
        "                validation_loss = self.evaluate(validation_loader)\n",
        "                self.__log({\"validation_loss\": validation_loss})\n",
        "            \n",
        "                rounded_loss = np.round(validation_loss, 3)\n",
        "                if rounded_loss > best_validation_loss:\n",
        "                    now = datetime.now().strftime(\"%H:%M:%S %d.%m.%Y\")\n",
        "                    checkpoint_path = f\"{rounded_loss}_{now}.pt\"\n",
        "\n",
        "                    checkpoint_info =  {\n",
        "                        \"epoch\": epoch+1,\n",
        "                        \"loss\": validation_loss\n",
        "                    }\n",
        "\n",
        "                    self.__make_checkpoint(info=checkpoint_info, path=checkpoint_path)\n",
        "\n",
        "                    best_validation_loss = rounded_loss\n",
        "                \n",
        "                if self.__scheduler is not None:\n",
        "                    self.__scheduler.step(validation_loss)\n",
        "                    \n",
        "            else:\n",
        "                if self.__scheduler is not None:\n",
        "                    self.__scheduler.step()\n",
        "            \n",
        "            lr = self.__optimizer.defaults[\"lr\"]\n",
        "            self.__log({\"lr\": lr})\n",
        "            \n",
        "    \n",
        "def collate_fn(batch):\n",
        "    images, labels = [], []\n",
        "    \n",
        "    for item in batch:\n",
        "        label, image = item.label, item.image.tolist()\n",
        "        \n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "        \n",
        "    return {\n",
        "        \"images\": torch.tensor(images),\n",
        "        \"labels\": torch.tensor(labels)\n",
        "    }\n",
        "\n",
        "\n",
        "def train_test_split(dataset, test_size=0.2):\n",
        "    length = len(dataset)\n",
        "    train_length = round(length * (1 - test_size))\n",
        "    test_length = length - train_length\n",
        "    \n",
        "    train_dataset, test_dataset = random_split(dataset, [train_length, test_length])\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "oIoODWc4biNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = FlowersDataset(path=config.path, augmentations=config.augmentations)\n",
        "rows, cols = 10, 10\n",
        "fig = plt.figure(figsize=(cols*3, rows*3))\n",
        "for _ in range(rows * cols):\n",
        "    item = dataset[_*40]\n",
        "    label = item.label\n",
        "    class_ = dataset.labels[label]\n",
        "    image = item.image.permute(1, 2, 0).numpy()\n",
        "    ax = fig.add_subplot(rows, cols, _+1)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(class_, fontsize=15, fontfamily=\"serif\", y=1.02)\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hSG6DWfZdlfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 1"
      ],
      "metadata": {
        "id": "oYyePadwn5qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center= False,\n",
        "                              samplewise_center= False,\n",
        "                              featurewise_std_normalization= False,\n",
        "                              samplewise_std_normalization=False,\n",
        "                              rotation_range= 10,        # 0- 180\n",
        "                              zca_whitening=False,\n",
        "                              zoom_range=0.1,            # Randomly zoom image\n",
        "                              width_shift_range=0.2,     # randomly shift images horizontally (fraction of total width)\n",
        "                              height_shift_range=0.2,    # randomly shift images vertically (fraction of total height)\n",
        "                              horizontal_flip=True,      # randomly flip images\n",
        "                              vertical_flip=False)       # randomly flip images\n",
        "                             \n",
        "datagen.fit(X_train)\n",
        "\n",
        "\n",
        "History = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n",
        "                              epochs = 50, validation_data = (X_test,y_test),\n",
        "                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size)\n",
        "\n",
        "Base_model = VGG16(include_top= False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Base_model)\n",
        "model.add(Dense(256,activation='relu'))\n",
        "# adding prediction(softmax) layer\n",
        "model.add(Dense(5,activation=\"softmax\"))\n",
        "\n",
        "Base_model.trainable = False\n",
        "\n"
      ],
      "metadata": {
        "id": "n_qffNH6dlcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred = my_model.predict(x_test)\n",
        "predicted = np.argmax(val_pred, axis=1)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "actual = np.argmax(y_test, axis=1)\n",
        "conf_mat = confusion_matrix(actual, predicted)\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cbar = False,  cmap = plt.cm.Blues)\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pmNzVseFvCtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(actual, predicted, target_names = CLASSES))"
      ],
      "metadata": {
        "id": "dWdtIknrvH17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 2"
      ],
      "metadata": {
        "id": "8A5Ln9Vyv7N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_generator = ImageDataGenerator(horizontal_flip=True,\n",
        "                                   width_shift_range = 0.4,\n",
        "                                   height_shift_range = 0.4,\n",
        "                                   zoom_range=0.3,\n",
        "                                   rotation_range=20,\n",
        "                                   )\n",
        "\n",
        "image_size = 224\n",
        "batch_size = 10\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "        '../input/flowers-recognition/flowers/flowers/',\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "Found 4323 images belonging to 5 classes.\n",
        "model = Sequential()\n",
        "\n",
        "model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "count = sum([len(files) for r, d, files in os.walk(\"../input/flowers-recognition/flowers/flowers/\")])\n",
        "\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=int(count/batch_size) + 1,\n",
        "        epochs=10)"
      ],
      "metadata": {
        "id": "QB7qgNo2vHzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}